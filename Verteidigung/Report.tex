\documentclass[11pt]{article}

\usepackage[table, svgnames, dvipsnames]{xcolor}
\usepackage[final]{acl}
% Download the ACL style files from the link below.
% You need the `acl.sty' and `acl_natbib.bst' files, keep these in the same
% directory as your TeX file.
% https://github.com/acl-org/acl-style-files/tree/master/latex
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
%\usepackage{inconsolata}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{amsmath}
%\bibliographystyle{acl}


\makeatletter
\def\endthebibliography{%
  \def\@noitemerr{\@latex@warning{Empty `thebibliography' environment}}%
  %\endlist
}
\makeatother


\title{Affective Speech Synthesis - A Comparison}

\author{Hannes Bachmann\\
  4847472\\
  INF BAS \\
  \texttt{\small hannes.bachmann@mailbox.tu-online.de}
  \\ \And
  Julia Rennert \\
  4924490\\
  INF-VERT2 \\
  \texttt{\small julia.rennert@tu-dresden.de}
  }
  
\definecolor{ashgrey}{rgb}{0.7, 0.75, 0.71}
\definecolor{gainsboro}{rgb}{0.86, 0.86, 0.86}
\definecolor{mintgreen}{rgb}{0.6, 1.0, 0.6}


\begin{document}
\maketitle
\begin{abstract}
Speech synthesis has become a common tool over the past few decades. This opens up the questions whether the synthesized speech can be given a more life-like sounding, namely emotionalism, a problem that is accumulated in the field of affective speech synthesis (emotional text-to-speech). \\
In our project, we compare modern technologies in emotion recognition and text-to-speech with common methodologies of acoustics and machine learning, using both to extract a sentence's prevalent emotion and transform it into emotional speech. We evaluate our methods in contrast to the common standard and find out whether our simplified approaches can compete.
\end{abstract}

\section{Introduction}
Speech synthesis has evolved from a difficult problem to a common tool over the past few decades.
It started with simulating mouth movements, then moved on to rule-based, concatenative and
statistical synthesis, and then to using deep learning. Today, synthesized speech is used in a
variety of applications, such as voice assistants and video dubbing.
While general speech synthesis has already made good progress, work is still being done on
how to give language a specific characteristic. To define this problem, the information content of
the spoken word is divided into three areas. First, there is the content of the text, more or less
a transcript of what is being said. Then there is the emotion that the speaker conveys through
his voice. And thirdly, there is the identity of the speaker himself, which is also characterized by
his voice. Affective Speech Synthesis refers to the change in emotion while the other two areas remain the static.

The most successful current approaches are based on deep learning\cite{triantafyllopoulos_overview_2023, cho_multi-speaker_2021, diatlova_emospeech_2023}. This warrants the question, whether a more traditional acousics-based approach can still compare to some extend. For this purpose, we tried to apply two competing approaches to the problem of emotional TTS, one up to the current deep learning methods, and one a mixture of acoustics and basic machine learning. 

In Section \ref{related_work}, we will discuss the basics of (affective) speech synthsis and the current research. Our own methodologies will be introduced in Section \ref{methodology}. Then we evaluate the methods in  Section \ref{evaluation} by comparing the outcomes to each other and two approaches of previous research, where one of them is a current deep learning approach and one a more traditional statistical method. We discuss the outcome in Section \ref{discussion} and summarize our findings in Section \ref{conclusion}.

\begin{table}[h]

\vspace{5px}
{
%\hspace{-15px}
\begin{tabular}{|l|l|l|}
\hline
\rowcolor{gainsboro}&our method&external method\\
\hline
\cellcolor{gainsboro}deep learning&Approach 1&EmoSpeech\\
\hline
\cellcolor{gainsboro}acoustic&Approach 2&Affect Editor\\
\hline

\end{tabular}

}
\caption{Goal of the evaluation}
\end{table}


\section{Related Work}
\label{related_work}
\subsection{Groundwork}
To understand the ongoings in the research area of affective speech synthesis, let’s first take a short look at regular speech synthesis. The beginning of speech synthesis lies in articulatory synthesis where the scientists tried to remodel the movements of the oral articulators. It was followed by the format synthesis where rule-based changes to amplitudes and frequencies of an oscillating source were made. Unfortunately, it turned out to be complicated to find the needed rules. This approach was followed by the concatenative speech synthesis where prerecorded phonemes, syllables or words were used to construct the speech. As this kind of synthesis needs a lot of prerecorded data and the output has lapses in continuity, it was abandoned for statistical parametric speech synthesis. This kind of synthesis consists of three stages, text analysis to find the correct pronunciation of the given words, the prediction of the speech parameters by using an acoustic model and the vocoding, the construction of the waveform. These stages could include machine learning, oftentimes hidden Markov models. With the upcoming of deep learning, this system was complemented by deep neuronal networks \cite{triantafyllopoulos_overview_2023,shen_natural_2018}.

Affective speech synthesis mostly uses the progress made in regular speech synthesis by utilizing its methods. The first affective speech synthesis algorithms were rule-based like in format synthesis. A very successful example for this type of synthesis is the Affect Editor of \cite{cahn_generation_2000}. Just as in regular speech synthesis, this method was followed by a concatenative approach, as for example used by \cite{pierre-yves_production_2003} to model emotional babbling. Finally, the affective speech synthesis reached a data-driven stage as well.
Nowadays emotional speech synthesis has two predominating methods. Text-to-emotional-features synthesis tries to extract the emotional utterances directly from the text, while emotional voice conversion goes the step over the regular speech synthesis and then applies the emotion to the pre-created speech. Due to the higher progress in normal speech synthesis research, the second approach is used more frequently.

Another distinguisher is made by the type of data that is available for the training. If there is a corpus of data from the same speaker and the same sentence with different underlying emotion, one speaks of parallel data. This kind of data is easy to learn from since you don’t have to distinguish between different speakers or different situations, but hard to get, as it mostly requires talented vocal artists to record it. Therefore, an active part of research is how to cope with non-parallel data, that is from different speakers or with different textual content.

\begin{figure}[h]
 \centering
\includegraphics[width=0.4\textwidth]{"Bilder/GAN.PNG"}
\caption{The two most prevalent GAN approaches as depicted by \cite{triantafyllopoulos_overview_2023}}
\end{figure}

A common method for emotional speech synthesis on parallel data are so-called GANs. They consist of two neuronal networks, the generator and the discriminator. The generator is given a sentence with a specific emotion that it needs to transform into another emotion. After this task is done, the transformed sentence is given to the discriminator which also receives a naturally spoken sentence with the same emotion. The task of the discriminator is to distinguish between the naturally spoken and the generates sentence. In the learning process, generator and discriminator continuously improve themselves. Improvements of this method are the CycleGAN, where two GANs translate between two different emotions such that the first GAN takes one emotion and transforms it into another emotion, and the second GAN takes this so generated emotion and transforms it back into the first emotion, and StarGAN where generator and discriminator are trained to work on multiple domains.

In order to work on non-parallel data, the disentanglement method comes into use. The influences on the voice are thereby split into the content, the emotion, and the speaker. Then different encoders are used to represent those influences.
Other demands of an affective speech synthesizer are that it controls the intensity of the emotion and that it allows a mixed emotion, for example a transition between sad and happy as it often happens in the mood swing of children. All in all, the synthesizers come to an accuracy of 50-80\% in studies. But their performance greatly depends on the environment, number and kind of emotions, language, culture, acceptance of user input and the resistance against noise \cite{triantafyllopoulos_overview_2023}.


\subsection{Competing Approaches}
\label{competing_approaches}
We will now take a look on some research in higher detail as to use it for the later evaluation. 

Our first comparison is to the Affect Editor of \cite{cahn_generation_2000}. As written in the previous section, Cahn's editor can be called the first real emotional TTS synthesizer. It encodes the emotion with the use of an acoustic model, where parameters encode important values like pitch, timing, voice quality and articulation. The inputs are an utterance and an emotion. After an initial acoustic analysis, the utterance is supplied with fitting pauses, hesitation and pitch to convey the emotion.

In comparison to Cahn's method, our simple approach uses a similar idea in terms of finding appropriate values to the most important acoustic values such as pitch, volume and timing. In contrast to the previously described method, the parameters are evaluated using machine learning instead of acoustic background knowledge. Thus, the primary contribution of the approach 2 is to search for a middle-way between acoustic knowledge and machine learning.

Our second comparison is to a novel approach by \cite{diatlova_emospeech_2023}. Based on the FastSpeech2 TTS architecture, they constructed their emotional TTS (EmoSpeech) model. FastSpeech2 is an encoder-decoder architecture which uses a feed-forward transformer block, a stack of multihead self-attention layers and 1D-convolution. With the use of embedding tables and FastSpeech2's eGeMAPS predictor, emotion is then projected on FastSpeech2's output on an utterance level.
\\

\todo{Vergleich zu Hannes' Ansatz}

The reason for the choice of these two papers is that one of them is a deep dive into the history of ETTS and the other one is a modern method using current deep learning technology. Above all that, they use similar human-based evaluation techniques that can be combined and compared to our approaches.

\section{Methodology}
\label{methodology}
In this section, we will explain our methodology. The basis is a method described by \cite{bohra_smart_2022}. The input is a single textual sentence. In the first step, the prevalent emotion is extracted with the help of a LLM. Then the text is transformed to (neutral) speech. Now starts the actual emotion embedding and our own work which is diverging from Bohra's method.

While the two approaches we followed differ widely, we can still subdivide them further into a pre-processing, model application and post-processing phase. During the pre-processing, the audios are transformed into sequences or values, that are less dependent on the actual audio. Those are fed into the neural network together with the emotion. The output is the transformation that the audio needs to undergo in order to display the wished emotion. In the final step, this transformation is undertaken.

\begin{figure}[h]
 \centering
\includegraphics[width=0.4\textwidth]{"Bilder/Prozess.PNG"}
\caption{Depiction of the general method of our two approaches}
\label{Ablauf}
\end{figure}

\subsection{Training and Validation Data}
We used two separate datasets to train and validate different parts of our data.
The first one is the \cite{saravia-etal-2018-carer} dataset. It contains 417 thousand sentences with annotated emotion labels. The annotated emotions are joy, sadness, fear, anger, love and surprise. We used it to validate the emotion recognition step of our procedure.

The second dataset is \cite{cao_data}, a dataset of 7,442 audio recordings taken by professional artists. The dataset consists of twelve sentences that are spoken by different people and with different emotions, ranging over neutral, anger, disgust, fear, happiness, and sadness. The value of this dataset lies in that the neutral audio clip supplies us with some kind of ground truth as it is comparable to the utterances, our TTS step produces. Thus we can train a model on the comparison between the neutral and the emotional sentences of each speaker.

In order to use emotions that both datasets know, we create the intersection of the emotions: Joy (happiness), sadness, anger, and fear. Additionally, we add a neutral category.
\subsection{Text-To-Speech}
Text-to-speech (TTS) deals with the problem of converting normal language text into spoken words mostly in the form of an audio signal. 
In the current development environment, there are multiple advanced TTS libraries like Tacotron2 by \cite{shen_natural_2018} from NVIDIA, Google text-to-speech (Gtts) \cite{gtts} or Azure AI speech of Microsoft. For simplicity reasons, we decided to use Gtts. Initially Gtts used a concatenative and parametric speech synthesis. In later versions it utilized WaveNet, a deep generative model developed by \citeauthor{van2016wavenet} from DeepMind which is able to directly generate raw audio waveforms from given text.
\subsection{Emotion Recognition}

The second ground stone of the information extraction is the emotion recognition. I used the Emotion English DistilRoBERTa-base\cite{hartmann2022emotionenglish}, which is trained to recognize six emotions: joy, sadness, anger, disgust, surprise and fear. The model performs well on the testing set in that it classifies 92\% of the emotions correctly. Only 19\% of the mismatched information are classified into the two emotion categories, that are not part of the input for our model. As for disgust we deal with it by classifying it as the most likely other known class, which is anger (87\%). Surprise is more differenciated, therefore it is evaluated further by enhancing the sentences with the allowed emotions and giving it to the model egain. If the model recognizes another emotion, this one is prefered, otherwise the sentence is given a neutral conotation. This improves the missinterpretation of surprise up to 73\%.

The whole procedure can be thus described as follows:
{
\small
\begin{equation*}
E(s)=\begin{cases}
  anger & \text{if } model(s)=disgust\\      
  max(model(emo_i+s)) & \text{if }  model(s)=surprise\\
  model(s) & \text{otherwise } 
\end{cases}
\end{equation*}
}
where $E$ is the emotion recognition algorithm, $s$ is the sentence model is the output of the Emotion English DistilRoBERTa-base, and $emo_i$ is a list of all allowed emotions integrated into the sentence "I am very *." in different variations.
This procedure pushes up the classifiaction accuracy to 93\%.

\subsection{Approach 1 - Hannes}
Our first approach is about using recent deep learning methods to generate an emotional audio. 
 
...
We carried out two experiments: \\
1. Training a transformer based model on prediction the emotional audio from a neutral one. \\
2. Using the TTS model XTTS-v2 by \cite{casanova2024xtts} and its voice cloning ability to mimic not only the speaker but also its emotion.\\
...
\subsubsection{Neutral-to-Emotional Transformer}
...
\subsubsection{Emotion Insertion using XTTS-v2}
...

\todo{Hannes' Ansatz}
\subsection{Approach 2}

Before we start out with the explaination of the simplified method, we should answer the question whether a simplification should be considered. The advantage of this methodology is its possibility to break down the problem into descriptive subproblems, which can be evaluated and optimized separately. It therefore increases the trustworthyness which is a general problem in the deep learning era.


\subsubsection{Embedding the Emotion into the Audio}

The sound of our speech is deeply correlated with acoustic effects which themselves stem from the mouth movement and form\cite{arias_beyond_2020}. 



Sadness is the easiest do encode as it has a distinct


\subsection{Implementation}

In order to implement a usable synthesizer, I created a GUI application. It consists of a text input for the sentence. With the click on a checkbox, the sentence is evaluated in order to find the needed emotion. If the checkbox is not selected, one can choose an arbitrary emotion instead. 

\begin{figure}[h]
 \centering
\includegraphics[width=0.4\textwidth]{"Bilder/GUI.png"}
\caption{GUI of the implementation}
\end{figure}

The sentence is then synthesized using the procedure in Figure \ref{Ablauf}. The complication is here to choose suitable python libraries to create and transform the audio clip. Google text-to-speech saves its output in a designated gtts class which has no options for transformation but can only be saved. In contrast, to get attributes like the volume or pitch of the audio or split it in segments, it needs to be transformed into another format. For volume and splitting, it suffices, to parse the audio into the AudioSegment format. Because the pitch is a complex attribute and not directly visible within the wave form of a audio, it needs more advanced functions, which can be found in the Librosia library. The same is to be said for stretching an audio, since a simple time stretch would not only change the length of the audio but also its frequencies and therefore the pitch.
Finally, the audio clip is ready. It is replayed and can be saved to a file.

\section{Evaluation}
\label{evaluation}

\subsection{Benchmark}
One of the ongoing problems of affective speech synthsis is that there isn't a global benchmark yet\cite{triantafyllopoulos_overview_2023}. Every paper uses their own methodology and testing dataset. In contrast, we want to propose a benchmark. The benchmark is separated in to two datasets containing 12 sentences each. Some of them are taken from the \cite{saravia-etal-2018-carer} dataset.

The first sentence collection contains simple sentences that don't have any subclauses. They are used to gather information about the general emotion representation in the spoken sentences. The second set of sentences contains sentences that have at least one subclause. The sentences are ordered by their lenght and should be used to prove that the algorithm can also work on non-standard sentences.

The sentences are then interlaced with the four emotions anger, happiness, sadness and fear. Additionally, there is a neutral category. One or more study participants listen to the sentences and annotate them with an emotion.


\subsection{Results}
\begin{table*}[t]

\centering
\vspace{5px}
{
\begin{tabular}{|p{2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|}
\hline
\rowcolor{mintgreen}&neutral&anger&happiness&sadness&fear&surprise&disgust\\
\hline
\cellcolor{gainsboro}Approach 1&&&&&&-&-\\
\hline
\cellcolor{gainsboro}Approach 2&&&&&&-&-\\
\hline
\hline
\cellcolor{gainsboro}Affect Editor& -&65.5&83.2&97.1&72.7&72.7&80.7\\
\hline
\cellcolor{gainsboro}EmoSpeech&56 &94&80 &83&-&100&-\\
\hline
\end{tabular}
}

\caption{Comparison between the quantitative evaluations of \cite{cahn_generation_2000}, \cite{diatlova_emospeech_2023} and our approaches (in percent).}
\label{Tabelle}
\end{table*}
\todo{Werte zu Tabelle hinzufügen}
In this section, we will use our benchmark to compare our approaches. We will also compare our outcomes to each other and to two approaches of other researchers, that were already described in Section \ref{competing_approaches}. You can refer to Table \ref{Tabelle} for the full benchmark data and similar data of the compared papers.

The metric of the comparison is a problem in itself, as ETTS has no uniform evaluation method or a benchmark. A common method of evaluation is to let people listen to the audios and then sort them according to the emotion they mean to hear. This is the method that is also used in our two reference papers. But the type and complexity of the sentences used is not given and metrices like clearity are not considered, so the comparison is rudimentary. This can also be seen in that the Affect Editor and EmoSpeech show comparable results despite their difference in up-to-dateness. Therefore, the evaluation has to be taken with a grain of salt.

The algorithms can have two sources of error. The first one lies in the emotion recognition. Based on the \cite{saravia-etal-2018-carer} dataset we can say, that the accuracy of this part lies at 93\%. The second risk of failure lies in the embedding of the emotion into the audio. Here our two approaches differ.

\todo{Evaluation Approach 1}

While approach 2 performs well in the quantitative analysis, it has the same qualitative problems that might compromise the affect editor. The emotion, while clearly embedded, tends to be comically emphasized, especially for the emotion fear.

The approach also doesn't scale well for complex sentences that contain subsentences.
\section{Discussion}
\label{discussion}
Let's first discuss our outcome.

Another important question is the general ethicality of improving the human-likeliness of synthetized speech. On the one hand, it improves the quality of generated speech. This could be very beneficial for both barrier improvement for blind people as well as translations, commercials, and artists (for example the dubbing of indie computer games). On the other hand, it allows for more believable deep fakes. It also poses the ever-present question of which data can be used for deep learning training without violating copyright. In our case, the data was taken from an open training set, but the given sets are sparse and as in our case too small to get optimal results\cite{he_improve_2022}. The risk of taking the needed training data from sources of questionable copyright such as audio books and quality such as noisy videos.

\section{Conclusion}
\label{conclusion}
It wasn't to be expected that a simple course project can hold up to the current hights scientific research.

Generally, Emotional TTS is constantly improving together with the advances in TTS. To name some fields that still warrant improvement, the evaluation of affective speech synthesizers is mostly still human-based and benchmarks don't exist yet. Finally, while single emotions can be embedded, a more differentiated mixture of maybe even conflicting emotions is not yet researched.

\section{Contribution statement}
Hannes Bachmann:
\begin{itemize}
\item text-to-speech
\item Approach 1 (main approach)
\end{itemize}
Julia Rennert:
\begin{itemize}
\item emotion recognition
\item Approach 2
\item find datasets, write "related work", benchmark
\end{itemize}

\section{Link to the Repository}

Please note that the data is not in the repository as the audio files are too big. To test out the algorithms that require the audios, please download them from the dataset source described above, and add them to the project in a "data" folder.

% Create a file `references.bib'
% 
% @article{dijkstra1968goto,
%   title={Go To Statement Considered Harmful (1968)},
%   inproceedings={CACM},
%   author={Dijkstra, Edsger},
%   year={2021}
% }

%\bibliographystyle{acl}
\bibliography{NLP}
Instructions: 
\begin{enumerate}


    \item ``Methodology'' lays out your technical approach, high-level, as well in technical detail. Subsections are highly recommended here (and also elsewhere).
    \item ``Evaluation'' should contain both a quantitative and a qualitative evaluation of your results. If in doubt about metrics and evaluation methodologies, talk to us.
    \item ``Discussion'' on the one hand builds upon the evaluation, and should critically discuss strengths and weaknesses of your solution, and possible ways to improve it further. On the other hand, it should discuss relevant ethical questions related to the problem and/or your solution at hand.
\end{enumerate}

\end{document}
